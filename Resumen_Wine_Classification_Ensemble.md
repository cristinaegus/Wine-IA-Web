# Resumen: ClasificaciÃ³n de Calidad de Vino con Modelos Ensemble

## ðŸ“Š InformaciÃ³n General del Proyecto

**Dataset:** Wine Quality (Vino Tinto)  
**Objetivo:** ClasificaciÃ³n multiclase de calidad de vino (escala 3-8)  
**TÃ©cnicas:** ComparaciÃ³n de modelos Bagging vs Boosting  
**Accuracy mÃ¡ximo alcanzado:** 97.5% (XGBoost)

---

## ðŸ”§ 1. PreparaciÃ³n del Entorno

### ConfiguraciÃ³n inicial:

- âœ… CreaciÃ³n de entorno virtual Python 3.12.10
- âœ… InstalaciÃ³n de librerÃ­as necesarias:
  - `pandas`, `numpy`: ManipulaciÃ³n de datos
  - `scikit-learn`: Machine Learning
  - `matplotlib`, `seaborn`: VisualizaciÃ³n
  - `xgboost`: Gradient Boosting avanzado
  - `imbalanced-learn`: Balanceo de clases
  - `kagglehub`: Descarga de datasets

### ImportaciÃ³n de datos:

- Archivo fuente: `winequality-red.csv`
- Dimensiones originales: 1,599 filas Ã— 12 columnas
- Variables: 11 caracterÃ­sticas fisicoquÃ­micas + 1 variable objetivo (quality)

---

## ðŸ“ˆ 2. AnÃ¡lisis Exploratorio de Datos (EDA)

### 2.1 InspecciÃ³n inicial:

- **Filas duplicadas:** 240 encontradas y eliminadas
- **Valores nulos:** 0 (dataset limpio)
- **DistribuciÃ³n de calidad:** Clases desbalanceadas (principalmente 5 y 6)

### 2.2 AnÃ¡lisis de correlaciones:

- **CorrelaciÃ³n positiva con calidad:** alcohol (0.48), sulphates (0.25)
- **CorrelaciÃ³n negativa con calidad:** volatile acidity (-0.39)
- **Sin multicolinealidad:** No hay caracterÃ­sticas altamente correlacionadas entre sÃ­

### 2.3 DistribuciÃ³n de clases originales:

- Calidad 3: 10 muestras (0.6%)
- Calidad 4: 53 muestras (3.3%)
- Calidad 5: 681 muestras (42.6%) âš ï¸
- Calidad 6: 638 muestras (39.9%) âš ï¸
- Calidad 7: 199 muestras (12.4%)
- Calidad 8: 18 muestras (1.1%)

---

## âš–ï¸ 3. Preprocesamiento de Datos

### 3.1 Balanceo de clases:

- **TÃ©cnica:** SMOTEENN (combinaciÃ³n de SMOTE + Edited Nearest Neighbours)
- **Resultado:** ~470 muestras por clase (distribuciÃ³n balanceada)
- **Dataset final:** 2,828 muestras

### 3.2 TransformaciÃ³n de etiquetas:

- **Problema:** XGBoost requiere etiquetas 0-5, no 3-8
- **SoluciÃ³n:** LabelEncoder para mapear calidades:
  - Calidad 3 â†’ 0, Calidad 4 â†’ 1, ..., Calidad 8 â†’ 5

### 3.3 DivisiÃ³n de datos:

- **Training:** 80% (2,262 muestras)
- **Testing:** 20% (566 muestras)
- **Estrategia:** train_test_split con random_state=42

### 3.4 Tratamiento de asimetrÃ­a:

- **Columnas asimÃ©tricas identificadas:** 6 variables
  - chlorides, residual sugar, total sulfur dioxide
  - sulphates, free sulfur dioxide, volatile acidity
- **TransformaciÃ³n:** PowerTransformer (Yeo-Johnson)

### 3.5 Escalado de caracterÃ­sticas:

- **TÃ©cnica:** MinMaxScaler (0-1)
- **AplicaciÃ³n:** Todas las caracterÃ­sticas despuÃ©s de transformaciones

---

## ðŸ¤– 4. Modelado y EvaluaciÃ³n

### 4.1 Modelos Base Individuales:

| Modelo                    | Accuracy | Observaciones             |
| ------------------------- | -------- | ------------------------- |
| Logistic Regression       | ~85%     | Modelo lineal base        |
| K-Nearest Neighbors       | 97.2%    | Excelente con k=1         |
| Support Vector Classifier | 97.2%    | Kernel RBF, C=100         |
| Decision Tree             | ~93%     | Susceptible a overfitting |

### 4.2 Modelos Ensemble Originales:

| Modelo            | Accuracy  | Tipo            |
| ----------------- | --------- | --------------- |
| Random Forest     | 95.9%     | Bagging         |
| Gradient Boosting | 97.2%     | Boosting        |
| **XGBoost**       | **97.5%** | **Boosting** ðŸ† |

---

## ðŸ†š 5. ComparaciÃ³n Detallada: Bagging vs Boosting

### 5.1 Modelos de Bagging:

| Modelo                   | Accuracy | DescripciÃ³n                                      |
| ------------------------ | -------- | ------------------------------------------------ |
| Bagging + Decision Trees | ~95%     | Bootstrap + Ã¡rboles paralelos                    |
| Random Forest            | ~96%     | Bagging + selecciÃ³n aleatoria de caracterÃ­sticas |
| Extra Trees              | ~95%     | MÃ¡s aleatorio que Random Forest                  |

**Promedio Bagging:** ~95.3%

### 5.2 Modelos de Boosting:

| Modelo            | Accuracy | DescripciÃ³n                  |
| ----------------- | -------- | ---------------------------- |
| AdaBoost          | ~92%     | Ajuste secuencial de pesos   |
| Gradient Boosting | ~97%     | MinimizaciÃ³n de gradiente    |
| XGBoost           | ~97.5%   | Gradient Boosting optimizado |

**Promedio Boosting:** ~95.5%

### 5.3 Resultado de la ComparaciÃ³n:

ðŸ† **GANADOR: Modelos de Boosting** (por margen mÃ­nimo)

---

## ðŸ“Š 6. Visualizaciones Creadas

### 6.1 AnÃ¡lisis exploratorio:

- âœ… Heatmap de correlaciones
- âœ… DistribuciÃ³n de clases (antes/despuÃ©s del balanceo)
- âœ… Histogramas y boxplots por caracterÃ­stica
- âœ… GrÃ¡ficos de barras: caracterÃ­sticas vs calidad

### 6.2 EvaluaciÃ³n de modelos:

- âœ… ComparaciÃ³n de accuracy por modelo
- âœ… Promedio Bagging vs Boosting
- âœ… Matrices de confusiÃ³n (Random Forest vs XGBoost)
- âœ… Reporte de clasificaciÃ³n detallado

---

## ðŸŽ¯ 7. Conclusiones Principales

### 7.1 Rendimiento de modelos:

1. **Mejor modelo individual:** XGBoost (97.5%)
2. **Mejores modelos base:** KNN y SVC (97.2%)
3. **Modelo mÃ¡s consistente:** Random Forest (96%+)

### 7.2 Bagging vs Boosting:

- **Boosting** ligeramente superior en promedio
- **Bagging** mÃ¡s estable y menos propenso a overfitting
- **Diferencia mÃ­nima:** ~0.2% entre enfoques

### 7.3 Factores clave del Ã©xito:

- âœ… Balanceo efectivo de clases con SMOTEENN
- âœ… TransformaciÃ³n adecuada de caracterÃ­sticas asimÃ©tricas
- âœ… Escalado apropiado de variables
- âœ… TransformaciÃ³n correcta de etiquetas para XGBoost

### 7.4 CaracterÃ­sticas mÃ¡s importantes:

- **Alcohol:** CorrelaciÃ³n mÃ¡s fuerte con calidad
- **Sulphates:** Segundo predictor mÃ¡s importante
- **Volatile acidity:** CorrelaciÃ³n negativa significativa

---

## ðŸ”§ 8. Aspectos TÃ©cnicos Destacados

### 8.1 DesafÃ­os resueltos:

- **Clases desbalanceadas:** SMOTEENN balanceÃ³ efectivamente
- **Etiquetas incompatibles:** LabelEncoder para XGBoost
- **CaracterÃ­sticas asimÃ©tricas:** PowerTransformer normalizÃ³ distribuciones
- **Escalas diferentes:** MinMaxScaler estandarizÃ³ rangos

### 8.2 Mejores prÃ¡cticas aplicadas:

- ValidaciÃ³n cruzada implÃ­cita en ensemble methods
- Uso de random_state para reproducibilidad
- DivisiÃ³n estratificada de datos
- EvaluaciÃ³n con mÃºltiples mÃ©tricas

### 8.3 Configuraciones Ã³ptimas:

- **Random Forest:** 100 estimadores, caracterÃ­sticas aleatorias
- **XGBoost:** learning_rate=0.1, max_depth=3, 100 estimadores
- **Gradient Boosting:** learning_rate=0.1, max_depth=3

---

## ðŸ“ 9. Archivos del Proyecto

```
ML Modelos Ensemble/
â”œâ”€â”€ Wine_multiclass_classification__97_5_accuracy.ipynb
â”œâ”€â”€ winequality-red.csv
â”œâ”€â”€ churn.csv
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .venv/
â””â”€â”€ Resumen_Wine_Classification_Ensemble.md
```

---

## ðŸš€ 10. PrÃ³ximos Pasos Sugeridos

### 10.1 Mejoras potenciales:

- [ ] Hyperparameter tuning con GridSearchCV
- [ ] Ensemble voting (combinaciÃ³n de mejores modelos)
- [ ] Feature engineering adicional
- [ ] ValidaciÃ³n cruzada estratificada explÃ­cita

### 10.2 ExperimentaciÃ³n adicional:

- [ ] Otros algoritmos de balanceo (ADASYN, BorderlineSMOTE)
- [ ] TÃ©cnicas de selecciÃ³n de caracterÃ­sticas
- [ ] Modelos de deep learning
- [ ] AnÃ¡lisis de importancia de caracterÃ­sticas

### 10.3 Deployment:

- [ ] CreaciÃ³n de API para predicciones
- [ ] ContainerizaciÃ³n con Docker
- [ ] Interfaz web con Streamlit/Flask
- [ ] Monitoreo de modelo en producciÃ³n

---

**Fecha de creaciÃ³n:** 8 de julio de 2025  
**Autor:** AnÃ¡lisis de clasificaciÃ³n de vino con modelos ensemble  
**Accuracy mÃ¡ximo alcanzado:** 97.5% con XGBoost  
**TÃ©cnicas clave:** Bagging, Boosting, SMOTEENN, PowerTransformer


About CSV

About this file
Context

This datasets is related to red variants of the Portuguese "Vinho Verde" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

The datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).

This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)

Content

For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)

Tips

What might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)

KNIME is a great tool (GUI) that can be used for this.
1 - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA.
2- File Reader to 'Rule Engine Node' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:

$quality$ > 6.5 => "good"
TRUE => "bad"
3- Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking)
4- Column Filter Node output to input of Partitioning Node (your standard train/tes split, e.g. 75%/25%, choose 'random' or 'stratified')
5- Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and
6- Partitioning Node test data split output to input Decision Tree predictor Node
7- Decision Tree learner Node output to input Decision Tree Node input
8- Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value)
Inspiration

Use machine learning to determine which physiochemical properties make a wine 'good'!

Acknowledgements

This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (I am mistaken and the public license type disallowed me from doing so, I will take this down at first request. I am not the owner of this dataset.

Please include this citation if you plan to use this database:
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

Relevant publication