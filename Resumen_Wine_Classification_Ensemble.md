# Resumen: Clasificaci√≥n de Calidad de Vino con Modelos Ensemble

## üìä Informaci√≥n General del Proyecto

**Dataset:** Wine Quality (Vino Tinto)  
**Objetivo:** Clasificaci√≥n multiclase de calidad de vino (escala 3-8)  
**T√©cnicas:** Comparaci√≥n de modelos Bagging vs Boosting  
**Accuracy m√°ximo alcanzado:** 97.5% (XGBoost)

---

## üîß 1. Preparaci√≥n del Entorno

### Configuraci√≥n inicial:

- ‚úÖ Creaci√≥n de entorno virtual Python 3.12.10
- ‚úÖ Instalaci√≥n de librer√≠as necesarias:
  - `pandas`, `numpy`: Manipulaci√≥n de datos
  - `scikit-learn`: Machine Learning
  - `matplotlib`, `seaborn`: Visualizaci√≥n
  - `xgboost`: Gradient Boosting avanzado
  - `imbalanced-learn`: Balanceo de clases
  - `kagglehub`: Descarga de datasets

### Importaci√≥n de datos:

- Archivo fuente: `winequality-red.csv`
- Dimensiones originales: 1,599 filas √ó 12 columnas
- Variables: 11 caracter√≠sticas fisicoqu√≠micas + 1 variable objetivo (quality)

---

## üìà 2. An√°lisis Exploratorio de Datos (EDA)

### 2.1 Inspecci√≥n inicial:

- **Filas duplicadas:** 240 encontradas y eliminadas
- **Valores nulos:** 0 (dataset limpio)
- **Distribuci√≥n de calidad:** Clases desbalanceadas (principalmente 5 y 6)

### 2.2 An√°lisis de correlaciones:

- **Correlaci√≥n positiva con calidad:** alcohol (0.48), sulphates (0.25)
- **Correlaci√≥n negativa con calidad:** volatile acidity (-0.39)
- **Sin multicolinealidad:** No hay caracter√≠sticas altamente correlacionadas entre s√≠

### 2.3 Distribuci√≥n de clases originales:

- Calidad 3: 10 muestras (0.6%)
- Calidad 4: 53 muestras (3.3%)
- Calidad 5: 681 muestras (42.6%) ‚ö†Ô∏è
- Calidad 6: 638 muestras (39.9%) ‚ö†Ô∏è
- Calidad 7: 199 muestras (12.4%)
- Calidad 8: 18 muestras (1.1%)

---

## ‚öñÔ∏è 3. Preprocesamiento de Datos

### 3.1 Balanceo de clases:

- **T√©cnica:** SMOTEENN (combinaci√≥n de SMOTE + Edited Nearest Neighbours)
- **Resultado:** ~470 muestras por clase (distribuci√≥n balanceada)
- **Dataset final:** 2,828 muestras

### 3.2 Transformaci√≥n de etiquetas:

- **Problema:** XGBoost requiere etiquetas 0-5, no 3-8
- **Soluci√≥n:** LabelEncoder para mapear calidades:
  - Calidad 3 ‚Üí 0, Calidad 4 ‚Üí 1, ..., Calidad 8 ‚Üí 5

### 3.3 Divisi√≥n de datos:

- **Training:** 80% (2,262 muestras)
- **Testing:** 20% (566 muestras)
- **Estrategia:** train_test_split con random_state=42

### 3.4 Tratamiento de asimetr√≠a:

- **Columnas asim√©tricas identificadas:** 6 variables
  - chlorides, residual sugar, total sulfur dioxide
  - sulphates, free sulfur dioxide, volatile acidity
- **Transformaci√≥n:** PowerTransformer (Yeo-Johnson)

### 3.5 Escalado de caracter√≠sticas:

- **T√©cnica:** MinMaxScaler (0-1)
- **Aplicaci√≥n:** Todas las caracter√≠sticas despu√©s de transformaciones

---

## ü§ñ 4. Modelado y Evaluaci√≥n

### 4.1 Modelos Base Individuales:

| Modelo                    | Accuracy | Observaciones             |
| ------------------------- | -------- | ------------------------- |
| Logistic Regression       | ~85%     | Modelo lineal base        |
| K-Nearest Neighbors       | 97.2%    | Excelente con k=1         |
| Support Vector Classifier | 97.2%    | Kernel RBF, C=100         |
| Decision Tree             | ~93%     | Susceptible a overfitting |

### 4.2 Modelos Ensemble Originales:

| Modelo            | Accuracy  | Tipo            |
| ----------------- | --------- | --------------- |
| Random Forest     | 95.9%     | Bagging         |
| Gradient Boosting | 97.2%     | Boosting        |
| **XGBoost**       | **97.5%** | **Boosting** üèÜ |

---

## üÜö 5. Comparaci√≥n Detallada: Bagging vs Boosting

### 5.1 Modelos de Bagging:

| Modelo                   | Accuracy | Descripci√≥n                                      |
| ------------------------ | -------- | ------------------------------------------------ |
| Bagging + Decision Trees | ~95%     | Bootstrap + √°rboles paralelos                    |
| Random Forest            | ~96%     | Bagging + selecci√≥n aleatoria de caracter√≠sticas |
| Extra Trees              | ~95%     | M√°s aleatorio que Random Forest                  |

**Promedio Bagging:** ~95.3%

### 5.2 Modelos de Boosting:

| Modelo            | Accuracy | Descripci√≥n                  |
| ----------------- | -------- | ---------------------------- |
| AdaBoost          | ~92%     | Ajuste secuencial de pesos   |
| Gradient Boosting | ~97%     | Minimizaci√≥n de gradiente    |
| XGBoost           | ~97.5%   | Gradient Boosting optimizado |

**Promedio Boosting:** ~95.5%

### 5.3 Resultado de la Comparaci√≥n:

üèÜ **GANADOR: Modelos de Boosting** (por margen m√≠nimo)

---

## üìä 6. Visualizaciones Creadas

### 6.1 An√°lisis exploratorio:

- ‚úÖ Heatmap de correlaciones
- ‚úÖ Distribuci√≥n de clases (antes/despu√©s del balanceo)
- ‚úÖ Histogramas y boxplots por caracter√≠stica
- ‚úÖ Gr√°ficos de barras: caracter√≠sticas vs calidad

### 6.2 Evaluaci√≥n de modelos:

- ‚úÖ Comparaci√≥n de accuracy por modelo
- ‚úÖ Promedio Bagging vs Boosting
- ‚úÖ Matrices de confusi√≥n (Random Forest vs XGBoost)
- ‚úÖ Reporte de clasificaci√≥n detallado

---

## üéØ 7. Conclusiones Principales

### 7.1 Rendimiento de modelos:

1. **Mejor modelo individual:** XGBoost (97.5%)
2. **Mejores modelos base:** KNN y SVC (97.2%)
3. **Modelo m√°s consistente:** Random Forest (96%+)

### 7.2 Bagging vs Boosting:

- **Boosting** ligeramente superior en promedio
- **Bagging** m√°s estable y menos propenso a overfitting
- **Diferencia m√≠nima:** ~0.2% entre enfoques

### 7.3 Factores clave del √©xito:

- ‚úÖ Balanceo efectivo de clases con SMOTEENN
- ‚úÖ Transformaci√≥n adecuada de caracter√≠sticas asim√©tricas
- ‚úÖ Escalado apropiado de variables
- ‚úÖ Transformaci√≥n correcta de etiquetas para XGBoost

### 7.4 Caracter√≠sticas m√°s importantes:

- **Alcohol:** Correlaci√≥n m√°s fuerte con calidad
- **Sulphates:** Segundo predictor m√°s importante
- **Volatile acidity:** Correlaci√≥n negativa significativa

---

## üîß 8. Aspectos T√©cnicos Destacados

### 8.1 Desaf√≠os resueltos:

- **Clases desbalanceadas:** SMOTEENN balance√≥ efectivamente
- **Etiquetas incompatibles:** LabelEncoder para XGBoost
- **Caracter√≠sticas asim√©tricas:** PowerTransformer normaliz√≥ distribuciones
- **Escalas diferentes:** MinMaxScaler estandariz√≥ rangos

### 8.2 Mejores pr√°cticas aplicadas:

- Validaci√≥n cruzada impl√≠cita en ensemble methods
- Uso de random_state para reproducibilidad
- Divisi√≥n estratificada de datos
- Evaluaci√≥n con m√∫ltiples m√©tricas

### 8.3 Configuraciones √≥ptimas:

- **Random Forest:** 100 estimadores, caracter√≠sticas aleatorias
- **XGBoost:** learning_rate=0.1, max_depth=3, 100 estimadores
- **Gradient Boosting:** learning_rate=0.1, max_depth=3

---

## üìÅ 9. Archivos del Proyecto

```
ML Modelos Ensemble/
‚îú‚îÄ‚îÄ Wine_multiclass_classification__97_5_accuracy.ipynb
‚îú‚îÄ‚îÄ winequality-red.csv
‚îú‚îÄ‚îÄ churn.csv
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .venv/
‚îî‚îÄ‚îÄ Resumen_Wine_Classification_Ensemble.md
```

---

## üöÄ 10. Pr√≥ximos Pasos Sugeridos

### 10.1 Mejoras potenciales:

- [ ] Hyperparameter tuning con GridSearchCV
- [ ] Ensemble voting (combinaci√≥n de mejores modelos)
- [ ] Feature engineering adicional
- [ ] Validaci√≥n cruzada estratificada expl√≠cita

### 10.2 Experimentaci√≥n adicional:

- [ ] Otros algoritmos de balanceo (ADASYN, BorderlineSMOTE)
- [ ] T√©cnicas de selecci√≥n de caracter√≠sticas
- [ ] Modelos de deep learning
- [ ] An√°lisis de importancia de caracter√≠sticas

### 10.3 Deployment:

- [ ] Creaci√≥n de API para predicciones
- [ ] Containerizaci√≥n con Docker
- [ ] Interfaz web con Streamlit/Flask
- [ ] Monitoreo de modelo en producci√≥n

---

**Fecha de creaci√≥n:** 8 de julio de 2025  
**Autor:** An√°lisis de clasificaci√≥n de vino con modelos ensemble  
**Accuracy m√°ximo alcanzado:** 97.5% con XGBoost  
**T√©cnicas clave:** Bagging, Boosting, SMOTEENN, PowerTransformer
